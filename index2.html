<html>
  <head>
    <meta charset="UTF-8">
    <title>端到端语音合成及其优化实践(下)</title>
  </head>
  <body>
    <article>
      <header>
        <h1>端到端语音合成及其优化实践(下)</h1>
      </header>
    </article>

    <div>
	  <h2>前言</h2>
<p>在<a href="https://zhuanlan.zhihu.com/p/37817418">端到端语音合成及其优化实践(上)</a> 。中，我们介绍了 Tacotron 以及 WaveNet，注音文本送入 Tacotron 输出 Mel-Spectrum， Mel-Spectrum 再送入 WaveNet 合成波形。但如前所述，自回归 WaveNet 性能堪忧，在 CPU 平台通常需要数十秒时间合成一秒语音。为了解决商业化问题，DeepMind 推出了 Parallel WaveNet，使 inference 速度提升上千倍，在 Google Cloud 上线、进驻 Duplex，取得巨大成功。</P>

<p>总体来讲，Parallel WaveNet 的复现很困难，主要体现在两方面，其一是 Parallel WaveNet 模型本身比较复杂，包含一个自回归的 WaveNet 以及一个并行的 IAF WaveNet，两个模型并不是联合训练的，用户需要预训练一个高质量的自回归 WaveNet，这是一个较高的门槛；其二是 Parallel WaveNet 使用 Knowledge Distillation 训练 IAF WaveNet， 实践中这种方式不稳定，需要其他 Loss 进行辅助，花费大量精力调参优化。针对这两个问题，我们推出了自己的方案 Uni-WaveNet，Uni-WaveNet 是一个不需要 Knowledge Distillation 的 Parallel WaveNet 方案，您没有看错，不需要 Knowledge Distillation。没有 Knowledge Distillation 意味着不需要预训练作为 “Teacher model” 的自回归 WaveNet 。具体而言，我们设计了一个深层 IAF WaveNet 结构，配合 Spectrogram Loss 进行逐层训练。这个过程是端到端的，摆脱了 Teacher Model。合成质量令人满意，并且模型自带从 "低质量+低实时率" 到 "高质量+高实时率" 的多种配置。</P>

<p>我们假设在阅读本文前，您已经了解过端到端语音合成的基本概念，包括 Tacotron、WaveNet 以及 Parallel WaveNet  等。如果需要补充相关知识，请先阅读我们的上篇介绍文章 <a href="https://zhuanlan.zhihu.com/p/37817418">端到端语音合成及其优化实践(上)</a>。</P>

	  <h2>Uni-WaveNet 实现方案</h2>
	  <h3>IAF：Inverse Autoregressive Flow</h3>
<p>在前言中我们提到， Uni-WaveNet 方案的核心是深层的 IAF WaveNet 结构，读者需要对 IAF (Inverse Autoregressive Flow) 有一定的了解，关于 Inverse Autoregressive Flow 的相关信息请参考 这篇文章 ，这里不再赘述。</P>
	  <div align="center"><img src="images/tts2_iaf.png"  alt="iaf" width="450px" height="300px"/></div>
	  <div align="center">图1: Inverse Autoregressive Flow</div>	
<p>通过阅读上面的文章，我们知道，IAF 解码快但训练慢。 为了解决训练速度问题，DeepMind 引入 Knowledge Distillation 直接拟合自回归 WaveNet 的生成分布，具体做法是预训练一个自回归 WaveNet，再通过采样计算 KL Loss 从0训练并行的 IAF WaveNet。但这样还不够，因为 Knowledge Distillation 非常脆弱，不易训练，所以还要配合 Spectrogram Loss，解决静音问题。这个训练过程工程味十足，不够优雅，那么有没有办法跳过 Knowledge Distillation 直接从 Spectrogram Loss 训练高质量的并行 IAF WaveNet 呢？这正是我们考虑的问题，我们一起看看 Spectrogram Loss 有多少潜力可挖。</P>
<p>首先介绍模型结构，为叙述方便，以下我们将这种方案简称为 Uni-WaveNet。</P>
	 
<p>遵循 IAF 框架，Uni-WaveNet 结构分为 WaveNet 和 IAF-WaveNet 两个层级。我们方案的设计主旨是尽量增加 IAF 深度，同时保持 WaveNet 性能不下降。我们知道 WaveNet 是一种基于 CNN 的采样点自回归模型，它的计算量出奇的大。在资源有限的前提下，增加 IAF 深度势必要缩减 WaveNet 的模型规模，进而造成 WaveNet 性能损失。所以，问题转换成如何在 IAF 深度和 WaveNet 模型规模间寻找平衡，使 Uni-WaveNet 总体性能最优。</p>
<p>Uni-WaveNet 的 WaveNet 和 IAF WaveNet 两个层配置如下。</P>
	  <h3>WaveNet 配置</h3>

<p>WaveNet 的性能主要取决于感受野宽度，感受野宽度正比与参数量，例如 WaveNet1 中使用扩张卷积，卷积核宽度为2 ，扩张系数每层 x2，大大提高了参数效率； Parallel WaveNet ，将卷积核宽度扩大为3，同时保持扩张系数每层 x2 不变，进一步提高了参数效率。</P>
<p>在我们的 WaveNet 方案中，卷积核宽度为 3，扩张系数每层 x3，参数效率比 Parallel WaveNet 更高。读者可能会认为扩张系数增长过快会影响模型性能，但根据我们的试验结果，这种担心是多余。</P>
	  <div align="center"><img src="images/tts2_wn.svg.png"  alt="tts2_wn.svg" width="600px" height="200px"/></div>
	  <div align="center">图2: Uni-WaveNet 中的 WaveNet</div>	
	  <div align="center"><img src="images/tts2_wn.png"  alt="tts2_wn" width="240px" height="200px"/></div>
	  <div align="center">图3: 标准 WaveNet</div>	
<p>为了更直观的对比，我们准备了图2和图3，分别代表 Uni-WaveNet 以及 WaveNet1，在 WaveNet1 中卷积核宽度为 2，扩张系数每层 x2， 每 10 层一组，共 3 组，组内扩张系数分别为  [1, 2, 4, 8, 16, 32, 64, 128, 256, 512] 。从图中我们可以看到，在网络层数相同的条件下，Uni-WaveNet 的感受野要大得多；或者反过来说，在感受野相当的条件下，Uni-WaveNet 的参数量更少，这使得我们有机会使用剩余的资源设计一种更深的 IAF 模型。</P>
	  <h3>IAF WaveNet 配置 </h3>
<p>我们设计了一种深层 IAF-WaveNet 架构，共 8 层，如图4。具体而言，IAF-WaveNet 包含 8层 WaveNet（记为 IAF1、IAF2...IAF8）。每层 IAF WaveNet 输出的波形分别计算 Spectrogram Loss，Spectrogram Loss 具体计算过程详见下后文。</P>
<p>对比 DeepMind Parallel WaveNet ，DeepMind Parallel WaveNet 采用 4 层 IAF 共 60 层 网络，前 3 层 IAF 采用 10 层 WaveNet，第 4 层 IAF 采用 30 层 WaveNet，卷积核宽度为 3 ，扩张系数每层 x2。Uni-WaveNet 的参数量与其相当（略少），感受野相当（除了最后一层 IAF），IAF 层数增加了 1 倍。</P>
	  <div align="center"><img src="images/tts2_uni_wn.png"  alt="tts2_uni_wn" width="1700px" height="800px"/></div>
	  <div align="center">图4: Uni-WaveNet 总体结构</div>	
	  <h3>Spectrogram 代价函数设计 </h3>

<p>在这一节中我们简要讨论一下 Spectrogram Loss 的设计。这是一个重要的问题，读者在尝试复现时必须小心设计 Spectrogram Loss ，因为合成质量对 Spectrogram Loss 十分敏感。总体来讲，组合窗长可以提高合成效果，短窗长减少抖动，长窗长提升音质。我们提出了一种 Spectrogram loss 计算方式，就与听觉特性平衡频谱能量对 loss 计算的代价。进一步，我们引入相位信息，以增强听感和稳定训练。</P>

	  <h3>训练和优化</h3>
<p>DeepMind Parallel WaveNet 和 Baidu ClariNet 只在最后一层计算 Loss，效果很好。但我们发现，当 IAF 深度达到 8 层时这样的训练方式难以为继，主要表现在，只有最后几层得到有效训练，输出呈现波形分布，而前几层基本是噪声。为此，我们提出多层 IAF 联合训练，有效的提高训练稳定性，提高收敛速度，合成语音抖动得到有效抑制，听感也提高了。能够有效挖掘出了深层 IAF 的模型能力。</P>

<p>多层 IAF 联合训练给我们带来一个额外的好处：因为 每层 IAF 的输出分布理论上是相同的，可以认为都是对真实波形的逼近，只是不同层的波形质量有所差异，所以我们可以抽取任意一层作为最终输出。这提高了灵活性，例如云端离线合成对实时率不敏感，可以输出最后一层波形，以获得最佳音质；云端在线或者移动端合成对实时率敏感，可以酌情剔除后面几层减小计算量。而值得注意的是，这种调整是不需要重训模型的，Uni-WaveNet 从结构上满足了不同应用场景对计算量的不同需求，自带从低音质+低实时率 到 高音质+高实时率的多种模式。</P>
	  <h3>IAF samples:</h3>

<p>从上到下依次为，Uni-WaveNet IAF1 到 Uni-WaveNet IAF8，以及 Autoregressive WaveNet，您可以直观感受不同 IAF 层的波形生成质量变化。</P>
 <table >
        <tr><td class="Uni-WaveNet_IAF" id="Uni-WaveNet_IAF"></td></tr>
		<tr><td>Uni-WaveNet IAF1</td>
        <td><audio controls><source src="samples/ttts2_iaf0.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF2</td>
        <td><audio controls><source src="samples/ttts2_iaf1.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF3</td>
        <td><audio controls><source src="samples/ttts2_iaf2.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF4</td>
        <td><audio controls><source src="samples/ttts2_iaf3.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF5</td>
        <td><audio controls><source src="samples/ttts2_iaf4.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF6</td>
        <td><audio controls><source src="samples/ttts2_iaf5.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF7</td>
        <td><audio controls><source src="samples/ttts2_iaf6.mp3"></audio></td></tr>
        <tr><td>Uni-WaveNet IAF8</td>
        <td><audio controls><source src="samples/ttts2_iaf7.mp3"></audio></td></tr>		
        <tr><td>Autoregressive WaveNet</td>
        <td><audio controls><source src="samples/ttts2_baseline_AR_wavenet.mp3"></audio></td></tr>		
      </table>
	  <h2>AB Test</h2>
<p>下面我们给出对比几组对比 Samples 分别出自 LSTM 参数系统、Tacotron + Griffin-Lim、Tacotron + WaveNet、Tacotron + Uni-WaveNet，大家可以试听一下作为对比。</P>
	<table >
	    <tr><td class="ab_test" id="ab_test"></td></tr>
	    <tr><td align="center">LSTM 参数合成</td>
		<td  align="center">Tacotron + Griffin-Lim</audio></td>
		<td  align="center">Tacotron + WaveNet</audio></td>
        <td  align="center">Tacotron + Uni-WaveNet</audio></td></tr>
        <tr><td><audio controls><source src="samples/ttts2_baseline_parameter.mp3"></audio></td>
        <td><audio controls><source src="samples/ttts2_baseline_Tacotron_GL.mp3"></audio></td>
		<td><audio controls><source src="samples/ttts2_baseline_AR_wavenet.mp3"></audio></td>
		<td><audio controls><source src="samples/ttts2_iaf7.mp3"></audio></td></tr>

    </table>
	  <h2>几个您可能关心的问题</h2>
<p>关于 ClariNet：</P>
<p>ClariNet 是一个很有意思的改进，用闭合形式计算 Knowledge Distillation，避免采样，简化了训练。我们确实进行了相关实验，在线性波形上 ClariNet 可以训练出高质量的波形，但在 μ-law 波形上 ClariNet 训练存在问题，主要是清音段波动大造成爆破音，我们分析原因是单高斯分布拟合能力有限。</P>
<p>Parallel WaveNet 是否可以复现自回归 WaveNet 的效果?</P>
<p>这是一个比较难回答的问题，但我们知道几个事实</P>
<p>1. 目前我们听到的所有并行 WaveNet Samples 效果都要或多或少略差于自回归 WaveNet</P>
<p>2. DeepMind Parallel WaveNet 宣称效果与自回归 WaveNet 持平，但如果您仔细观察官方 Samples 的谱图，会发现还是有高频损失的。但这样也并不能说明 DeepMind 夸大合成效果，因为测听有主观因素。</p>
<p>3. 上述 2 点都是只在耳机环境下对比，如果切换到外放场景，那么两者的差距会迅速缩小，甚至听不出来差异。所以还要看播放设备以及实际的应用场景。</P>
<p>未来的改进点：</p>
<p>Spectrogram Loss 还有大量有价值的改进的方向，作为抛砖引玉，我们这里只提一点：是相位特征。虽然在本文的 Spectrogram Loss 中，我们已经间接引入了相位，但形式上还比较简单、粗糙。</p>
	  <h2>参考文献</h2>
	  <p>1. <a href="https://arxiv.org/pdf/1609.03499.pdf">WaveNet: A Generative Model for Raw Audio</a></p>
      <p>2. <a href="https://arxiv.org/pdf/1711.10433.pdf">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</a></p>
	  <p>3. <a href="https://arxiv.org/pdf/1807.07281.pdf">ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech</a></p>
